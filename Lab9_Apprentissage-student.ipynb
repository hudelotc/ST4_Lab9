{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche d'Information et traitement de données massives\n",
    "\n",
    "## Lab 9 : Apprentissage et Recherche d'information\n",
    "## Learning to Rank : Approches par paires et Représentations distribuées \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette séance est double. Une première partie consistera en la mise en oeuvre de l'approche d'apprentissage pour l'ordonnancement par paires de préférences et la deuxième partie portera sur l'utilisation d'approches d'apprentissage de représentations distribuées pouvant être utilisées pour faire de l'expansion de requêtes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation de l'environnement.\n",
    "\n",
    "Plusieurs bibliothèques sont nécessaires pour ce TP. \n",
    "Les commandes ci-dessous vous permettent de les installer et de les importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy\n",
    "! pip install scipy\n",
    "! pip install matplotlib\n",
    "! pip install scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import datasets\n",
    "from sklearn import svm,linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 1 : Learning to rank - Approches par paires\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Génération d'un jeu de données synthétique\n",
    "\n",
    "Pour ce TP et afin de bien comprendre le principe de la transformée par paires, nous allons commencer par travailler sur un jeu de données synthétiques qu'il faudra générer.\n",
    "En particulier, nous allons nous placer dans un cas où nous considérons un ensemble de 60 échantillons $X$ décrit selon 2 dimensions et dont les valeurs de pertinence sont entières, i.e. $Y = \\{0, 1, 2\\}$. \n",
    "\n",
    "Nous considérerons que ces données appartiennent à deux requêtes en divisant l'échantillon en deux parties égales telles que $X = X_1 \\cup X_2$. Chaque partie est générée selon des distributions normales différentes (utilisation de la fonction [`randn`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html?highlight=randn#numpy.random.randn) de numpy).\n",
    "\n",
    "Le code ci-dessous permet de générer ce jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "theta = np.deg2rad(60)\n",
    "w = np.array([np.sin(theta), np.cos(theta)])\n",
    "K = 20\n",
    "X = np.random.randn(K, 2)\n",
    "y = [0] * K\n",
    "for i in range(1, 3):\n",
    "    X = np.concatenate((X, np.random.randn(K, 2) + i * 4 * w))\n",
    "    y = np.concatenate((y, [i] * K))\n",
    "\n",
    "# Petit déplacement des éléments de la seconde partition\n",
    "X[::2] -= np.array([3, 7]) \n",
    "# Création des partitions\n",
    "blocks = np.array([0, 1] * (X.shape[0] / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de X\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de y \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce jeu de données va maintenant être séparé en jeu de données d'apprentissage et jeu de données de test à l'aide de scikit-learn et de son module `model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(test_size=.5)\n",
    "cv.get_n_splits(X, y)\n",
    "for train_index, test_index in cv.split(X,y):\n",
    "    X_train,X_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]\n",
    "    b_train,b_test=blocks[train_index],blocks[test_index]\n",
    "\n",
    "idx = (b_train == 0)\n",
    "\n",
    "print(X_train[idx])\n",
    "print(X_train[idx,0])\n",
    "y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant afficher graphiquement le jeu de données d'apprentissage en utilisant des ronds pour les échantillons de X1 et des trianges pour les échantillons de X2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (b_train == 0)\n",
    "plt.scatter(X_train[idx, 0], X_train[idx, 1], c=y_train[idx], \n",
    "    marker='^', cmap=plt.cm.Reds,s=100)\n",
    "plt.scatter(X_train[~idx, 0], X_train[~idx, 1], c=y_train[~idx],\n",
    "    marker='o', cmap=plt.cm.Reds, s=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faites de même pour le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que remarquez-vous ?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer le code ci-dessous. Cela devrait faciliter votre réponse à la question précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (b_train == 0)\n",
    "plt.scatter(X_train[idx, 0], X_train[idx, 1], c=y_train[idx], \n",
    "    marker='^', cmap=plt.cm.Reds,s=100)\n",
    "plt.scatter(X_train[~idx, 0], X_train[~idx, 1], c=y_train[~idx],\n",
    "    marker='o', cmap=plt.cm.Reds, s=100)\n",
    "plt.arrow(0, 0, 8 * w[0], 8 * w[1], fc='gray', ec='gray', \n",
    "    head_width=0.5, head_length=0.5)\n",
    "plt.text(0, 1, '$w$', fontsize=20)\n",
    "plt.arrow(-3, -8, 8 * w[0], 8 * w[1], fc='gray', ec='gray', \n",
    "    head_width=0.5, head_length=0.5)\n",
    "plt.text(-2.6, -7, '$w$', fontsize=20)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faites le même travail pour le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimation de w\n",
    "Nous allons essayer d'estimer $w$ à l'aide d'un modèle linéaire. En particulier, nous vous demandons pour cela d'utiliser une régression régularisée (méthode [`linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) de scikit_learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher le vecteur estimé avec les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que remarquez-vous ?** Quelles sont  vos conclusions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimation de la qualité du modèle obtenu.\n",
    "\n",
    "\n",
    "Pour évaluer la qualité de notre modèle, nous devons définir une note de classement. Comme c'est l'ordre des données qui nous intéresse, il est naturel de choisir une mesure qui compare l'ordre de notre modèle à l'ordre donné. Pour cela, nous utiliserons le coefficient de corrélation [`tau de Kendall`](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), qui est défini comme $\\frac{P - Q}{P + Q}$, $P$ étant le nombre de paires concordantes et $Q$ le nombre de paires discordantes. Cette mesure est largement utilisée dans la littérature de classement (voir par exemple cet [article](http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf)). \n",
    "\n",
    "Cette mesure est définie dans scipy [stats.kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)\n",
    "\n",
    "Appliquer cette mesure pour évaluer le modèle linéaire appris sur chaque bloc séparemment et en utilisant le jeu de données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qu'en deduisez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformée par paires\n",
    "\n",
    "Comme nous l'avons vu dans le cours et comme cela a été montrée dans le [papier de Herbrich](https://www.mendeley.com/catalogue/a2709c3c-0705-3058-89db-28aeab2161f2/), si l'on considère des fonctions de classement linéaire, le problème de classement peut être transformé en un problème de classement à deux classes. Pour cela, nous formons la différence de tous les éléments comparables de telle sorte que nos données sont transformées en $(x'_k, y'_k) = (x_i - x_j, signe(y_i - y_j))$ pour toutes les paires comparables.\n",
    "\n",
    "De cette façon, nous nous ramenons à un problème de classement à deux classes. \n",
    "\n",
    "Appliquer cette transformation à votre jeu de données. Nous vous recommandons pour cela d'utiliser le module [`itertools`](https://docs.python.org/3.7/library/itertools.html#itertools.combinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher le nouveau jeu de données ainsi constitué en apprentissage et en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que remarquez-vous ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation du classifieur\n",
    "\n",
    "Nous allons maintenant utiliser un SVM sur les données transformées. Nous utiliserons pour cela le module [`svm`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) de sklearn. \n",
    "\n",
    "Appliquer cela à votre nouveau jeu de données d'apprentissage et récupérer les valeurs du $w$ estimé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les données d'apprentissage initiales avec le coefficient estimé $\\hat{w}$ par votre modèle.Ce modèle est connu sous le nom de [RankSVM](https://en.wikipedia.org/wiki/Ranking_SVM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne vous reste plus qu'à appliquer la mesure de Kendall à ce nouveau modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que remarquez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approche par paires sur le jeu de données du Lab8 (optionnel)\n",
    "\n",
    "Il s'agit ici de mettre en place l'approche par paires sur le jeu de données du Lab8. La principale difficulté est de mettre en place la transformation dite pairwise qui permet de représenter vos données comme des paires de préférence. \n",
    "\n",
    "Ecrire le code permettant de transformer vos données pour pouvoir utiliser cette approche. Vous pourrez pour cela utiliser la bibliothèque scikit-learn et [itertools](https://docs.python.org/fr/3/library/itertools.html) pour créer facilement vos différentes paires.\n",
    "Il suffira ensuite d'utiliser un séparateur linéaire de type SVM comme dans l'exercice précédent avec les données synthétiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2 : Représentations distribuées (source : l'excellent cs224n)\n",
    "***\n",
    "\n",
    "L'objectif de cette partie est de mettre en oeuvre les approches dites de plongement lexical et qui permettent de mieux capturer la sémantique des mots et ainsi d'améliorer la mise en correspondance entre requêtes et documents.\n",
    "Pour commencer, installer les modules nécessaires et configurer votre environnement. Nous utiliserons notamment ici la bibliothèque [`gensim`](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('reuters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approche 1 : A l'aide du comptage de mots\n",
    "\n",
    "\n",
    "La plupart des modèles de plongement de mots partent de l'idée suivante :\n",
    "\n",
    "*You shall know a word by the company it keeps* ([Firth, J. R. 1957:11](https://en.wikipedia.org/wiki/John_Rupert_Firth))\n",
    "\n",
    "Ainsi la plupart des approches sont motivées par l'idée que des mots similaires, c'est-à-dire des synonymes (proches), seront utilisés dans des contextes similaires. En conséquence, des mots similaires seront souvent prononcés ou écrits avec un sous-ensemble de mots partagés, c'est-à-dire des contextes. En examinant ces contextes, nous pouvons essayer de développer des templates pour nos mots. Avec cette intuition, de nombreuses approches se basent donc sur le comptage des mots. Vous développerez ici l'une de ces stratégies, les matrices de cooccurrence (pour plus d'informations, voir [ici](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Co-Occurrence\n",
    "\n",
    "Une matrice de cooccurrence compte la fréquence de cooccurrence des mots dans un environnement donné. Si un mot $w_i$ apparaît dans un document, nous considérons la *fenêtre contextuelle* qui entoure $w_i$. En supposant que la taille de notre fenêtre fixe est $n$, il s'agit alors des $n$ mots précédents et $n$ mots suivants dans ce document, c'est-à-dire les mots $w_{i-n} \\dots w_{i-1}$ et $w_{i+1} \\dots w_{i+n}$. Nous  pouvons ainsi construire une *matrice de cooccurrence* $M$, qui est une matrice symétrique mot par mot dans laquelle $M_{ij}$ est le nombre de fois que $w_j$ apparaît dans la fenêtre de $w_i$ parmi tous les documents.\n",
    "\n",
    "Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "Document 2: \"all is well that ends well\"\n",
    "\n",
    "\n",
    "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
    "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
    "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
    "\n",
    "**Note:** En NLP, les tokens `<START>` et `<END>` servent à representer le debut et la fin de phrases, paragraphes et documents.\n",
    "\n",
    "Les lignes (ou colonnes) de cette matrice fournissent un type de vecteurs de mots (ceux basés sur la cooccurrence mot-mot), mais les vecteurs seront en général grands (linéaires dans le nombre de mots distincts dans un corpus).\n",
    "\n",
    "Il est donc nécessaire de faire une étape de *réduction de la dimensionnalité*. En particulier, on peut ici utiliser une *SVD (Singular Value Decomposition)*, qui est une sorte de *PCA (Principal Components Analysis)* généralisée pour sélectionner les composantes principales de $k$ comme illustré ci-dessous.\n",
    "\n",
    "![Image d'une SVD](./Figures/svd.png)\n",
    "\n",
    "Cette représentation de co-occurrence à dimension réduite préserve les relations sémantiques entre les mots, par exemple *doctor* et *hospital* seront plus proches que *doctor* et *dog*. \n",
    "\n",
    "Quelques ressources concernant cette méthode :\n",
    "+ [ici](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf). \n",
    "+ [ici](https://web.stanford.edu/class/cs168/l/l9.pdf) \n",
    "\n",
    "En pratique, il est difficile d'appliquer uneSVD complète à de grands corpus en raison de la mémoire nécessaire. Cependant, comme nous ne voulons que les composantes vectorielles les plus importantes pour des $k$ relativement petits, on pourra utiliser la [Truncated SVD] (https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD) - pour laquelle il existe des techniques raisonnablement efficaces pour les calculer de manière itérative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Application à Reuters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Nous allons maintenant appliquer cette approche au corpus Reuters disponible dans `nltk`. Le corpus se compose de 10 788 documents d'actualité totalisant 1,3 million de mots. Ces documents couvrent 90 catégories et sont répartis en deux catégories : train et test. Plus de détails [ici](https://www.nltk.org/book/ch02.html). la fonction `read_corpus` ci-dessous permet de ne retirer que les articles de la catégorie \"brut\" (c'est-à-dire les articles d'actualité sur le pétrole, le gaz, etc). La fonction ajoute également des jetons `<START>` et `<END>` à chacun des documents, ainsi que des mots en minuscules. Vous n'avez **pas** à effectuer d'autres types de prétraitement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(category=\"crude\"):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite l'appliquer et avoir une vue sur quelques documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_corpus = read_corpus()\n",
    "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.1: Ecrire `distinct_words` \n",
    "\n",
    "Completer la fonction ci-dessous permettant de récupérer les mots uniques ou le vocabulaire du corpus. Vous pouvez pour cela utiliser des méthodes de python sur les listes comme décrit [ici](https://coderwall.com/p/rcmaea/flatten-a-list-of-lists-in-one-line-in-python) ou [ici](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html).\n",
    "\n",
    "De même les [Python sets](https://www.w3schools.com/python/python_sets.asp) peuvent être utiles pour supprimer les doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous vous permet de tester votre fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# Note that this not an exhaustive check for correctness.\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
    "\n",
    "# Correct answers\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
    "ans_num_corpus_words = len(ans_test_corpus_words)\n",
    "\n",
    "# Test correct number of words\n",
    "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
    "\n",
    "# Test correct words\n",
    "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.2:  Implementer `compute_co_occurrence_matrix` \n",
    "\n",
    "Écrire une méthode qui construit une matrice de cooccurrence pour une certaine taille de fenêtre $n$ (avec une valeur par défaut de 4), en considérant les mots $n$ avant et $n$ après le mot au centre de la fenêtre. Penser à utiliser `numpy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
    "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester avec le code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# Note that this is not an exhaustive check for correctness.\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and get student's co-occurrence matrix\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
    "\n",
    "# Correct M and word2Ind\n",
    "M_test_ans = np.array( \n",
    "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
    "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
    "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
    "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
    "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
    "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
    "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
    "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
    "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
    "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
    ")\n",
    "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
    "word2Ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
    "\n",
    "# Test correct word2Ind\n",
    "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n",
    "\n",
    "# Test correct M shape\n",
    "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
    "\n",
    "# Test correct M values\n",
    "for w1 in word2Ind_ans.keys():\n",
    "    idx1 = word2Ind_ans[w1]\n",
    "    for w2 in word2Ind_ans.keys():\n",
    "        idx2 = word2Ind_ans[w2]\n",
    "        student = M_test[idx1, idx2]\n",
    "        correct = M_test_ans[idx1, idx2]\n",
    "        if student != correct:\n",
    "            print(\"Correct M:\")\n",
    "            print(M_test_ans)\n",
    "            print(\"Your M: \")\n",
    "            print(M_test)\n",
    "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.3 : Mettre en oeuvre le code \"reduce_to_k_dim\" \n",
    "\n",
    "Construire une méthode qui effectue une réduction de dimensionnalité sur la matrice à k dimensions. Utiliser la SVD pour prendre les k plus grandes composantes et produire une nouvelle matrice à k dimensions. \n",
    "\n",
    "**Note** : Tous les logiciels numpy, scipy et scikit-learn (`sklearn`) fournissent une *certaine* implémentation de la SVD, mais seuls scipy et sklearn fournissent une implémentation de la SVD tronquée, et seul sklearn fournit un algorithme randomisé efficace pour calculer la SVD tronquée à grande échelle. Veuillez donc utiliser [sklearn.decomposition.TruncatedSVD] (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "    \n",
    "    \n",
    "        # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester votre code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# Note that this is not an exhaustive check for correctness \n",
    "# In fact we only check that your M_reduced has the right dimensions.\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and run student code\n",
    "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
    "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
    "\n",
    "# Test proper dimensions\n",
    "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
    "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.4 : Afficher les plongement : \"Plot_embeddings\" \n",
    "\n",
    "Ici, vous allez écrire une fonction les plongements obtenus à l'aide de matplotlib.\n",
    "\n",
    "Pour cet exemple, il peut être utile d'adapter [ce code] (https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/). Penser à cela à l'avenir et pour l'EI : regarder [la galerie Matplotlib] (https://matplotlib.org/gallery/index.html), de trouver un graphique qui ressemble un peu à ce que vous voulez, et adapter le code qu'ils donnent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words):\n",
    "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
    "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\n",
    "        Include a label next to each point.\n",
    "        \n",
    "        Params:\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
    "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
    "            words (list of strings): words whose embeddings we want to visualize\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester votre code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# Note that this is not an exhaustive check for correctness.\n",
    "# The plot produced should look like the \"test solution plot\" depicted below. \n",
    "# ---------------------\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Outputted Plot:\")\n",
    "\n",
    "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
    "word2Ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
    "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
    "plot_embeddings(M_reduced_plot_test, word2Ind_plot_test, words)\n",
    "\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1.5 : Analyse des plongements obtenus\n",
    "\n",
    "Nous allons maintenant rassembler toutes les parties précédente.\n",
    "Il s'agit de calculer la matrice de cooccurrence avec une fenêtre fixe de 4 (la taille de fenêtre par défaut), sur le corpus \"brut\" (pétrole) de Reuters. Ensuite, nous utiliserons TruncatedSVD pour calculer les encastrements bidimensionnels de chaque mot. TruncatedSVD retourne U\\*S, nous devons donc normaliser les vecteurs retournés, de sorte que tous les vecteurs apparaissent autour du cercle unitaire (donc la proximité est la proximité directionnelle). \n",
    "\n",
    "**Note** : La ligne de code ci-dessous qui effectue la normalisation utilise le concept NumPy de *Broadcasting*. Si vous ne connaissez pas le concept de Broadcasting, consultez\n",
    "[Computation on Arrays : Broadcasting by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html).\n",
    "\n",
    "Exécuter le code ci-dessous. Cela prendra probablement quelques secondes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Run This Cell to Produce Your Plot\n",
    "# ------------------------------\n",
    "reuters_corpus = read_corpus()\n",
    "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
    "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
    "\n",
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "\n",
    "plot_embeddings(M_normalized, word2Ind_co_occurrence, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les resultats obtenus et notamment les regroupements obtenus** Comment pourriez-vous utiliser cela dans le cadre de la RI ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approche 2 : Vecteurs de mots basés sur des prédictions  (optionnel)\n",
    "\n",
    "Comme rapidement éviqué en cours, des vecteurs de mots plus récents basés sur des prédictions ont montré de meilleures performances, tels que word2vec et GloVe. Ici, nous allons explorer les encastrements produits par GloVe. C.f. [l'article original de GloVe] (https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "Exécuter le code ci-dessous pour charger les vecteurs GloVe en mémoire. \n",
    "\n",
    "**Note** : Si c'est la première fois que vous exécutez ces cellules, c'est-à-dire que vous téléchargez le modèle, l'exécution prendra environ 15 minutes. Si vous avez déjà exécuté ces cellules auparavant, les relancer chargera le modèle sans le recharger, ce qui prendra environ 1 à 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take several minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réduire la dimensionnalité des plongements \n",
    "Comparons directement les plongements de GloVe à ceux de la matrice de co-occurrence. Afin d'éviter de manquer de mémoire, nous travaillerons plutôt avec un échantillon de 10000 vecteurs GloVe.\n",
    "Exécutez les cellules suivantes pour :\n",
    "\n",
    "1. Mettre 10000 vecteurs GloVe dans une matrice M\n",
    "2. Exécutez reduce_to_k_dim (votre fonction SVD tronquée) pour réduire les vecteurs de 200 dimensions à 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']):\n",
    "    \"\"\" Put the GloVe vectors into a matrix M.\n",
    "        Param:\n",
    "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
    "        Return:\n",
    "            M: numpy matrix shape (num words, 200) containing the vectors\n",
    "            word2Ind: dictionary mapping each word to its row number in M\n",
    "    \"\"\"\n",
    "    import random\n",
    "    words = list(wv_from_bin.vocab.keys())\n",
    "    print(\"Shuffling words ...\")\n",
    "    random.seed(224)\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        if w in words:\n",
    "            continue\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
    "# Note: This should be quick to run\n",
    "# -----------------------------------------------------------------\n",
    "M, word2Ind = get_matrix_of_vectors(wv_from_bin)\n",
    "M_reduced = reduce_to_k_dim(M, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
    "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.1 : Analyse des plongements GloVe\n",
    "\n",
    "Lancez la cellule ci-dessous pour tracer les plongements GloVe en 2D pour \"[\"barils\", \"bpd\", \"ecuador\", \"energy\", \"industry\", \"kuwait\", \"oil\", \"output\", \"petroleum\", \"venezuela\"]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "plot_embeddings(M_reduced_normalized, word2Ind, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'est-ce qui se regroupe dans l'espace bidimensionnel ? Qu'est-ce qui ne se regroupe pas et qui, selon vous, devrait l'être ? En quoi le tracé est-il différent de celui généré précédemment à partir de la matrice de cooccurrence ? Quelle est la raison possible de cette différence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.2 : Mots à sens multiples \n",
    "\n",
    "Les polysèmes et les homonymes sont des mots qui ont plus d'une signification (voir cette [page wiki] (https://en.wikipedia.org/wiki/Polysemy) pour en savoir plus sur la différence entre les polysèmes et les homonymes). Trouvez un mot ayant au moins deux significations différentes de sorte que les dix mots les plus similaires (selon la similarité du cosinus) contiennent des mots apparentés ayant les deux significations. For example, \"leaves\" a \"vanishes\" et \"stalks\" in the top 10, et \"scoop\" a \"handed_waffle_cone\" et \"lowdown\". Vous devrez probablement essayer plusieurs mots polysémiques ou homonymes avant d'en trouver un. Veuillez indiquer le mot que vous découvrez et les multiples significations qui figurent dans le top 10. Pourquoi pensez-vous que plusieurs des mots polysémiques ou homonymiques que vous avez essayés n'ont pas fonctionné (c'est-à-dire que les 10 mots les plus similaires du top ne contiennent qu' **une** des significations des mots) ?\n",
    "\n",
    "**Note** : Utiliser la fonction `wv_from_bin.most_similar(word)` pour obtenir le top 10 des mots les plus similaires. Cette fonction classe tous les autres mots du vocabulaire en fonction de leur similarité cosinus avec le mot donné. Pour plus d'informations, [documentation GenSim](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "    # Write your implementation here.\n",
    "    \n",
    "    \n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Résoudre les analogies avec les vecteurs de mots\n",
    "\n",
    "Il a été démontré que les vecteurs de mots *sont parfois* capables de résoudre des analogies. \n",
    "\n",
    "À titre d'exemple, pour l'analogie \"homme : roi : : femme : x\" (lire : l'homme est au roi comme la femme est à x), qu'est-ce que x ?\n",
    "\n",
    "Dans la cellule ci-dessous, nous vous montrons comment utiliser les vecteurs de mots pour trouver x. La fonction \"le plus similaire\" trouve les mots qui sont les plus similaires aux mots de la liste \"positive\" et les plus différents des mots de la liste \"négative\". La réponse à l'analogie sera le mot classé le plus similaire (la plus grande valeur numérique).\n",
    "\n",
    "**Note:** Une documentation supplémentaire sur la fonction `most_similar` peut être trouvée \n",
    "dans la [documentation GenSim](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- man : king :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.4 : Trouver des analogies \n",
    "\n",
    "Trouvez un exemple d'analogie. Dans votre solution, veuillez indiquer l'analogie complète sous la forme x:y : : a:b. \n",
    "\n",
    "**Note** : Vous devrez peut-être essayer de nombreuses analogies pour en trouver une qui fonctionne !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
